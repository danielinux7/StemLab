{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we begin, run this cell if you are using Colab\n",
    "!git clone -b 3-ysi-tutorial https://github.com/nestauk/im-tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Scrape the main table of UK's Yearly Box Office using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/intl/uk/yearly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the webpage content\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML page\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the relevant table\n",
    "table = soup.find_all('table')[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and store the data of every table row\n",
    "lst = []\n",
    "for row in table.find_all('tr'):\n",
    "    s = pd.Series([data.text for data in row.find_all('td')])\n",
    "    lst.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data in a Pandas DataFrame and place the first row of the DataFrame as header. Use the .head() method to check your DataFrame.\n",
    "data = pd.concat(lst, axis=1).T\n",
    "\n",
    "# Grab the first row for the header\n",
    "new_header = data.iloc[0]\n",
    "\n",
    "# Take the data less the header row\n",
    "data = data[1:]\n",
    "\n",
    "# Set the header row as the df header\n",
    "data.columns = new_header\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "data.Distributor.value_counts().plot(kind='bar', title='Films per distributor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "\n",
    "# Remove the £ symbol and transform strings to integers\n",
    "data['Gross'] = data['Gross'].apply(lambda x: int(x[1:].replace(',', '')))\n",
    "\n",
    "# Group the data by Distributor and add the Gross value of their movies\n",
    "data.groupby('Distributor')['Gross'].sum().sort_values(ascending=False).plot(kind='bar', title='Gross earning by distributor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Use Selenium to scrape Box Office Mojo's top #100 for every year between 2002 and 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN THIS CELL WHEN USING THE NOTEBOOK LOCALLY - YOU SHOULD INSTALL SELENIUM FIRST\n",
    "# import selenium.webdriver\n",
    "# # Path to the Chrome driver for my Mac -- yours will differ\n",
    "# mac_path = '../../chromedriver'\n",
    "# driver = selenium.webdriver.Chrome(executable_path=mac_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN THIS CELL WHEN USING THE NOTEBOOK ON COLAB - NO PREVIOUS INSTALLATION OF SELENIUM IS NEEDED\n",
    "# # install chromium, its driver, and selenium\n",
    "# !apt update\n",
    "# !apt install chromium-chromedriver\n",
    "# !pip install selenium\n",
    "# # set options to be headless\n",
    "# from selenium import webdriver\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--no-sandbox')\n",
    "# options.add_argument('--disable-dev-shm-usage')\n",
    "# # open it, go to a website, and get results\n",
    "# driver = webdriver.Chrome('chromedriver',options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2df(source, q):\n",
    "    \"\"\"A wrapper of the scraping pipeline we used before.\"\"\"\n",
    "    # Parse the HTML page\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    # Choose the relevant table\n",
    "    table = soup.find_all('table')[4]\n",
    "\n",
    "    # Parse and store the data of every table row\n",
    "    lst = []\n",
    "    for row in table.find_all('tr'):\n",
    "        s = pd.Series([data.text for data in row.find_all('td')])\n",
    "        lst.append(s)\n",
    "\n",
    "    # Concatenate the data in a Pandas DataFrame and place the first row of the DataFrame as header.\n",
    "    data = pd.concat(lst, axis=1).T\n",
    "\n",
    "    # Grab the first row for the header\n",
    "    new_header = data.iloc[0]\n",
    "\n",
    "    # Take the data less the header row\n",
    "    data = data[1:]\n",
    "\n",
    "    # Set the header row as the df header\n",
    "    data.columns = new_header\n",
    "    \n",
    "    # Add a new column tagging the page we scraped\n",
    "    data['page'] = q \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/intl/uk/yearly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to use in Selenium\n",
    "driver.get(url)\n",
    "\n",
    "lst = []\n",
    "# Loop over the years.\n",
    "for i in range(2001, 2019):\n",
    "    year = str(i + 1)\n",
    "    # Locate Hyperlinks by partial link text\n",
    "    elem = driver.find_element_by_partial_link_text(year)\n",
    "    # Click on the next page\n",
    "    elem.click()\n",
    "    # Store the Pandas DataFrame with the scraped content in a list\n",
    "    lst.append(html2df(driver.page_source, year))\n",
    "\n",
    "# Concatenate all Pandas DataFrames\n",
    "annual_top_100 = pd.concat(lst, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_top_100.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'% OF MISSING VALUES PER COLUMN\\n{(annual_top_100.isnull().sum() / annual_top_100.shape[0]) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'(MOVIES, COLUMNS) -> {annual_top_100.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37]",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
