{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjIE4q49Yi69"
      },
      "outputs": [],
      "source": [
        "# Before we begin, run this cell if you are using Colab\n",
        "!git clone https://github.com/danielinux7/StemLab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9ptwgOYi7K"
      },
      "source": [
        "# Web Scraping\n",
        "\n",
        "## Content\n",
        "1. HTML pages\n",
        "2. Chrome DevTools\n",
        "3. Web scraping packages\n",
        "    * BeautifulSoup\n",
        "        * Exercise\n",
        "    * Selenium\n",
        "        * Exercise\n",
        "4. Ethical considerations of web scraping\n",
        "\n",
        "## What you will be able to do after the tutorial\n",
        "* Inspect an HTML page and identify which parts you want to scrape.\n",
        "* Scrape web pages with `requests` and `BeautifulSoup`.\n",
        "* Navigate Javascript elements with `Selenium`\n",
        "* Judge when web scraping is the most suitable approach and what you should consider before doing so (be a good citizen of the Internet).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTML page structure\n",
        "\n",
        "**Hypertext Markup Language (HTML)** is the standard markup language for documents designed to be displayed in a web browser. HTML describes the structure of a web page and it can be used with **Cascading Style Sheets (CSS)** and a scripting language such as **JavaScript** to create interactive websites. HTML consists of a series of elements that \"tell\" to the browser how to display the content. Lastly, elements are represented by **tags**.\n",
        "\n",
        "Here are some tags:\n",
        "* `<!DOCTYPE html>` declaration defines this document to be HTML5.  \n",
        "* `<html>` element is the root element of an HTML page.  \n",
        "* `<div>` tag defines a division or a section in an HTML document. It's usually a container for other elements.\n",
        "* `<head>` element contains meta information about the document.  \n",
        "* `<title>` element specifies a title for the document.  \n",
        "* `<body>` element contains the visible page content.  \n",
        "* `<h1>` element defines a large heading.  \n",
        "* `<p>` element defines a paragraph.  \n",
        "* `<a>` element defines a hyperlink.\n",
        "\n",
        "HTML tags normally come in pairs like `<p>` and `</p>`. The first tag in a pair is the opening tag, the second tag is the closing tag. The end tag is written like the start tag, but with a slash inserted before the tag name.\n",
        "\n",
        "![](https://github.com/danielinux7/StemLab/blob/master/3-Web-Scraping/figures/tags.png?raw=1)\n",
        "\n",
        "HTML has a tree-like 🌳 🌲 structure thanks to the **Document Object Model (DOM)**, a cross-platform and language-independent interface. Here's how a very simple HTML tree looks like.\n",
        "\n",
        "![](https://github.com/danielinux7/StemLab/blob/master/3-Web-Scraping/figures/dom_tree.gif?raw=1)\n"
      ],
      "metadata": {
        "id": "SfDTLwsDawwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a simple HTML page"
      ],
      "metadata": {
        "id": "olFox1_TbLTG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a-7CLI_Yi7Z"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWldglTVYi7b"
      },
      "outputs": [],
      "source": [
        "display(HTML(\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\" dir=\"ltr\">\n",
        "<head>\n",
        "  <title>Intro to HTML</title>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "  <h1>Heading h1</h1>\n",
        "  <h2>Heading h2</h2>\n",
        "  <h3>Heading h3</h3>\n",
        "  <h4>Heading h4</h4>\n",
        "\n",
        "  <p>\n",
        "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
        "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
        "  </p>\n",
        "\n",
        "  <p>\n",
        "    This <br> is a paragraph <br> with <br> line breaks\n",
        "  </p>\n",
        "\n",
        "  <p style=\"color:red\">\n",
        "    Add colour to your paragraphs.\n",
        "  </p>\n",
        "\n",
        "  <p>Unordered list:</p>\n",
        "  <ul>\n",
        "    <li>Python</li>\n",
        "    <li>R</li>\n",
        "    <li>Julia</li>\n",
        "  </ul>\n",
        "\n",
        "  <p>Ordered list:</p>\n",
        "  <ol>\n",
        "    <li>Data collection</li>\n",
        "    <li>Exploratory data analysis</li>\n",
        "    <li>Data analysis</li>\n",
        "    <li>Policy recommendations</li>\n",
        "  </ol>\n",
        "  <hr>\n",
        "\n",
        "  <!-- This is a comment -->\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWuKFO4cYi7f"
      },
      "source": [
        "## Chrome DevTools: exercise\n",
        "\n",
        "[Chrome DevTools](https://developers.google.com/web/tools/chrome-devtools/) is a set of web developer tools built directly into the Google Chrome browser. DevTools can help you view and edit web pages. We will use Chrome's tool to inspect an HTML page and find which elements correspond to the data we might want to scrape.\n",
        "\n",
        "### Short exercise\n",
        "To get some experience with the HTML page structure and Chrome DevTools, we will search and locate elements in [Apsnypress](https://www.apsnypress.info/). \n",
        "\n",
        "**Tip**: Hit *Command+Option+C* (Mac) or *Control+Shift+C* (Windows, Linux) to access the elements panel.\n",
        "\n",
        "#### Tasks (we will do them together)\n",
        "* Find the flags images in the main page.\n",
        "* Find the headers in the news list.\n",
        "* Find the links in the pagination stripe.\n",
        "* Locate one of the photos from the news list.\n",
        "* Locate the read more link from the news list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJbdSl5QYi7h"
      },
      "source": [
        "# `BeautifulSoup`\n",
        "\n",
        "We will use `requests` and `BeautifulSoup` to access and scrape the content of [Sputnik Abkhazia](https://sputnik-abkhazia.info). We need to scrape parallel articles in Russian and Abkhazian, then build a prallel corpus of text.\n",
        "\n",
        "### What is `BeautifulSoup`?\n",
        "\n",
        "It is a Python library for pulling data out of HTML and XML files. It provides methods to navigate the document's tree structure that we discussed before and scrape its content.\n",
        "\n",
        "### Our pipeline\n",
        "\n",
        "![](https://github.com/danielinux7/StemLab/blob/master/3-Web-Scraping/figures/scrape-pipeline2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QL4XooVWYi7j"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b93vh8x8Yi7n"
      },
      "outputs": [],
      "source": [
        "# Sputnik's homepage for news in Abkhazia\n",
        "url_ru = \"https://sputnik-abkhazia.ru/Abkhazia\"\n",
        "url_ab = \"https://sputnik-abkhazia.info\"\n",
        "\n",
        "# Doing some research, it seems their website is grouping articles that were released for everyday.\n",
        "# We need to add YYYYMMDD i.e 20220609\n",
        "ymd = \"20220609\"\n",
        "\n",
        "# Use requests to retrieve data from a given URL\n",
        "sputnik_response_ru = requests.get(url_ru+\"/\"+ymd)\n",
        "sputnik_response_ab = requests.get(url_ab+\"/\"+ymd)\n",
        "\n",
        "# Parse the whole HTML page using BeautifulSoup\n",
        "sputnik_soup_ru = BeautifulSoup(sputnik_response_ru.text, 'html.parser')\n",
        "sputnik_soup_ab = BeautifulSoup(sputnik_response_ab.text, 'html.parser')\n",
        "\n",
        "# Title of the parsed page\n",
        "print(sputnik_soup_ru.title)\n",
        "print(sputnik_soup_ab.title)\n",
        "# sputnik_soup.title.string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThdMML74Yi7p"
      },
      "outputs": [],
      "source": [
        "# The HTML web page, here we Chrome DevTools to better understand the page.\n",
        "# i.e https://sputnik-abkhazia.ru/Abkhazia/20220609/\n",
        "print(sputnik_soup_ru.prettify())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_NlK1PxYi75"
      },
      "source": [
        "### Find links\n",
        "\n",
        "In many cases, it is useful to collect the links contained in a webpage (for example, you might want to scrape them too). Here is how you can do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrHi2D-0NGuh"
      },
      "outputs": [],
      "source": [
        "# Collect first link from the articles list\n",
        "first_link = sputnik_soup_ab.find('a', {'class': 'list__title'})\n",
        "print(first_link.prettify())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX0MEzdnNGuj"
      },
      "outputs": [],
      "source": [
        "first_link.get('href')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fepgivZHUtuo"
      },
      "outputs": [],
      "source": [
        "# Find all links\n",
        "links_ru = []\n",
        "links_ab = []\n",
        "\n",
        "# for loop\n",
        "# for link in sputnik_soup_ab.find_all('a', {'class': 'list__title'}):\n",
        "#   links.append(link.get('href'))\n",
        "\n",
        "# for loop in one line (list comprehension: create a new list based on the values of an existing list)\n",
        "links_ru = [link.get('href') for link in sputnik_soup_ru.find_all('a', {'class': 'list__title'})]\n",
        "links_ab = [link.get('href') for link in sputnik_soup_ab.find_all('a', {'class': 'list__title'})]\n",
        "\n",
        "# Add homepage and keep the unique links\n",
        "fixed_links_ru = list([''.join([url_ru, link]) for link in links_ru if link])\n",
        "fixed_links_ab = list([''.join([url_ab, link]) for link in links_ab if link])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_gkG8tTUtuq"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)\n",
        "pp.pprint(fixed_links_ab)\n",
        "print()\n",
        "pp.pprint(fixed_links_ru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfkPXwcNYi77"
      },
      "source": [
        "### Collect articles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all articles by requesting the pages with their links.\n",
        "article__texts_ab = []\n",
        "article__texts_ru = []\n",
        "\n",
        "for link in fixed_links_ab:\n",
        "  article_response = requests.get(link)\n",
        "  article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "  article__texts_ab.append(article_soup.find_all('div', {'class': 'article__text'}))\n",
        "\n",
        "for link in fixed_links_ru:\n",
        "  article_response = requests.get(link)\n",
        "  article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "  article__texts_ru.append(article_soup.find_all('div', {'class': 'article__text'}))"
      ],
      "metadata": {
        "id": "YkE9A3GgU2Wk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We print the articles in the standard output\n",
        "for article__text in article__texts_ab:\n",
        "  [print(text.getText()) for text in article__text]\n",
        "  print()"
      ],
      "metadata": {
        "id": "K_BTRhoNwREp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We print the articles in two files\n",
        "import sys\n",
        "\n",
        "origin_stdout = sys.stdout\n",
        "sys.stdout = open(\"ab.txt\", \"w\")\n",
        "for article__text in article__texts_ab:\n",
        "  [print(text.getText()) for text in article__text]\n",
        "  print()\n",
        "\n",
        "sys.stdout = open(\"ru.txt\", \"w\")\n",
        "for article__text in article__texts_ru:\n",
        "  [print(text.getText()) for text in article__text]\n",
        "  print()\n",
        "sys.stdout = origin_stdout"
      ],
      "metadata": {
        "id": "XOiDDhZCS6yT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pEn52EPYi8g"
      },
      "source": [
        "## Advanced web scraping tools\n",
        "\n",
        "**[Scrapy](https://scrapy.org)** is a Python framework for large scale web scraping. It gives you all the tools you need to efficiently extract data from websites, process them as you want, and store them in your preferred structure and format.\n",
        "\n",
        "**[ARGUS](https://github.com/datawizard1337/ARGUS)** is an easy-to-use web mining tool that's built on Scrapy. It is able to crawl a broad range of different websites.\n",
        "\n",
        "**[Selenium](https://selenium-python.readthedocs.io/index.html)** is an umbrella project encapsulating a variety of tools and libraries enabling web browser automation. Selenium specifically provides infrastructure for the W3C WebDriver specification — a platform and language-neutral coding interface compatible with all major web browsers. We can use it to imitate a user's behaviour and interact with Javascript elements (buttons, sliders etc.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OMqapDQYi8r"
      },
      "source": [
        "## Ethical considerations\n",
        "\n",
        "**You can scrape it, should you though?**\n",
        "\n",
        "A very good summary of practices for [ethical web scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01):\n",
        "\n",
        "* If you have a public API that provides the data I’m looking for, I’ll use it and avoid scraping all together.\n",
        "* I will only save the data I absolutely need from your page.\n",
        "* I will respect any content I do keep. I’ll never pass it off as my own.\n",
        "* I will look for ways to return value to you. Maybe I can drive some (real) traffic to your site or credit you in an article or post.\n",
        "* I will respond in a timely fashion to your outreach and work with you towards a resolution.\n",
        "* I will scrape for the purpose of creating new value from the data, not to duplicate it.\n",
        "\n",
        "Some other [important components](http://robertorocha.info/on-the-ethics-of-web-scraping/) of ethical web scraping practices include:\n",
        "\n",
        "* Read the Terms of Service and Privacy Policies of a website before scraping it (this might not be possible in many situations though).\n",
        "* If it’s not clear from looking at the website, contact the webmaster and ask if and what you’re allowed to harvest.\n",
        "* Be gentle on smaller websites\n",
        "    * Run your scraper in off-peak hours\n",
        "    * Space out your requests.\n",
        "* Identify yourself by name and email in your User-Agent strings.\n",
        "* Inspecting the **robots.txt** file for rules about what pages can be scraped, indexed, etc.\n",
        "\n",
        "### What is a robots.txt?\n",
        "\n",
        "A simple text file placed on the web server which tells crawlers which file they can and cannot access. It's also called _The Robots Exclusion Protocol_.\n",
        "\n",
        "![](https://github.com/danielinux7/StemLab/blob/master/3-Web-Scraping/figures/robots.png?raw=1)\n",
        "\n",
        "#### Some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfIa5-xQYi8s"
      },
      "outputs": [],
      "source": [
        "print(requests.get('https://sputnik-abkhazia.info/robots.txt').text)\n",
        "\n",
        "# Check out apsnypress's robots file on their site\n",
        "# https://apsnypress.info/robots.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO7uTnnhYi8u"
      },
      "source": [
        "#### What's a User-Agent?\n",
        "\n",
        "A User-Agent is a string identifying the browser and operating system to the web server. It's your machine's way of saying _Hi, I am Chrome on macOS_ to a web server.\n",
        "\n",
        "Web servers use user agents for a variety of purposes:\n",
        "* Serving different web pages to different web browsers. This can be used for good – for example, to serve simpler web pages to older browsers – or evil – for example, to display a “This web page must be viewed in Internet Explorer” message.\n",
        "* Displaying different content to different operating systems – for example, by displaying a slimmed-down page on mobile devices.\n",
        "* Gathering statistics showing the browsers and operating systems in use by their users. If you ever see browser market-share statistics, this is how they’re acquired.\n",
        "\n",
        "Let's break down the structure of a human-operated User-Agent:\n",
        "\n",
        "```Mozilla/5.0 (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Mobile/7B405```\n",
        "\n",
        "The components of this string are as follows:\n",
        "\n",
        "* Mozilla/5.0: Previously used to indicate compatibility with the Mozilla rendering engine.\n",
        "* (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us): Details of the system in which the browser is running.\n",
        "* AppleWebKit/531.21.10: The platform the browser uses.\n",
        "* (KHTML, like Gecko): Browser platform details.\n",
        "* Mobile/7B405: This is used by the browser to indicate specific enhancements that are available directly in the browser or through third parties. An example of this is Microsoft Live Meeting which registers an extension so that the Live Meeting service knows if the software is already installed, which means it can provide a streamlined experience to joining meetings.\n",
        "\n",
        "When scraping websites, it is a good idea to include your contact information as a custom **User-Agent** string so that the webmaster can get in contact. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_mqp0YoYi8v"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Nart Tlisha bot',\n",
        "    'From': 'daniel.abzakh@gmail.com'\n",
        "}\n",
        "request = requests.get('https://apsnypress.info', headers=headers)\n",
        "print(request.request.headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYzianchYi8w"
      },
      "source": [
        "## Additional resources/references:\n",
        "\n",
        "* [Document Object Model (DOM)](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction)\n",
        "* [HTML elements reference guide](https://www.w3schools.com/tags/default.asp)\n",
        "* [About /robots.txt](https://www.robotstxt.org/robotstxt.html)\n",
        "* [The robots.txt file](https://varvy.com/robottxt.html)\n",
        "* [Ethics in Web Scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n",
        "* [On the Ethics of Web Scraping](http://robertorocha.info/on-the-ethics-of-web-scraping/)\n",
        "* [User-Agent](https://en.wikipedia.org/wiki/User_agent)\n",
        "* [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "* [Selinium Python - Unofficial documentation](https://selenium-python.readthedocs.io/)\n",
        "* [ARGUS paper](http://ftp.zew.de/pub/zew-docs/dp/dp18033.pdf)\n",
        "* [Brian's C. Keegan](http://www.brianckeegan.com/) excellent [5-week web scraping course](https://github.com/CU-ITSS/Web-Data-Scraping-S2019) intended for researchers in the social sciences and humanities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:py36]",
      "language": "python",
      "name": "conda-env-py36-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "web-scraping.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}