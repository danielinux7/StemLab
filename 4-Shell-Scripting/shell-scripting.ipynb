{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjIE4q49Yi69"
      },
      "outputs": [],
      "source": [
        "# Before we begin, run this cell if you are using Colab\n",
        "!git clone https://github.com/danielinux7/StemLab.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu9ptwgOYi7K"
      },
      "source": [
        "# Shell Scripting\n",
        "\n",
        "#### Content\n",
        "1. Shell commands (shuf, wc, grep, sed, sort, uniq, cat, head, tail)\n",
        "\n",
        "#### What you will be able to do after the tutorial\n",
        "* Extract text from pdf (data dump).\n",
        "* Text clean up, remove undesired output.\n",
        "* Split text into sentences.\n",
        "* Build a parallel corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Extract text from pdf (data dump)**"
      ],
      "metadata": {
        "id": "drhxXSQE4Dzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "j30qh3tf4Xo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "import PyPDF2\n",
        "import sys\n",
        "\n",
        "# store standard output to restore it later\n",
        "origin_stdout = sys.stdout\n",
        "\n",
        "# creating a pdf file object\n",
        "pdfFileObj_ab = open('/content/StemLab/4-Shell-Scripting/last-of-the-departed_ ab.pdf', 'rb')\n",
        "pdfFileObj_ru = open('/content/StemLab/4-Shell-Scripting/last-of-the-departed_ ru.pdf', 'rb')\n",
        "\n",
        "# creating a pdf reader object\n",
        "pdfReader_ab = PyPDF2.PdfFileReader(pdfFileObj_ab)\n",
        "pdfReader_ru = PyPDF2.PdfFileReader(pdfFileObj_ru)\n",
        "\n",
        "# extracting text from pdf\n",
        "sys.stdout = open(\"ab.txt\", \"w\")\n",
        "for i in range(pdfReader_ab.numPages):\n",
        "    current_page = pdfReader_ab.getPage(i)\n",
        "    print(current_page.extractText())\n",
        "\n",
        "sys.stdout = open(\"ru.txt\", \"w\")\n",
        "for i in range(pdfReader_ru.numPages):\n",
        "    current_page = pdfReader_ru.getPage(i)\n",
        "    print(current_page.extractText())\n",
        "    \n",
        "sys.stdout = origin_stdout\n",
        "# closing the pdf file object\n",
        "pdfFileObj.close()\n"
      ],
      "metadata": {
        "id": "wQjdvSDS241_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Text clean up, remove undesired output**\n",
        "\n",
        "The extracted text from pdf files need a lot of clean up, first thing to do is to look at the txt files, to firgure out similiar noise patterns.\n",
        "\n",
        "Eventually we will use shell commands and scripting to accomplish our goal, the reason for using shell commands because they have fast execution time, this is very import in big data."
      ],
      "metadata": {
        "id": "4PLJncum92-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove empty lines with sed"
      ],
      "metadata": {
        "id": "3-ZCshJvARQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -40 ab.txt | sed -r '/^[ ]*$/d'"
      ],
      "metadata": {
        "id": "Ey5r19YqBNdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYzianchYi8w"
      },
      "source": [
        "## Additional resources/references:\n",
        "\n",
        "* [Document Object Model (DOM)](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction)\n",
        "* [HTML elements reference guide](https://www.w3schools.com/tags/default.asp)\n",
        "* [About /robots.txt](https://www.robotstxt.org/robotstxt.html)\n",
        "* [The robots.txt file](https://varvy.com/robottxt.html)\n",
        "* [Ethics in Web Scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n",
        "* [On the Ethics of Web Scraping](http://robertorocha.info/on-the-ethics-of-web-scraping/)\n",
        "* [User-Agent](https://en.wikipedia.org/wiki/User_agent)\n",
        "* [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "* [Selinium Python - Unofficial documentation](https://selenium-python.readthedocs.io/)\n",
        "* [ARGUS paper](http://ftp.zew.de/pub/zew-docs/dp/dp18033.pdf)\n",
        "* [Brian's C. Keegan](http://www.brianckeegan.com/) excellent [5-week web scraping course](https://github.com/CU-ITSS/Web-Data-Scraping-S2019) intended for researchers in the social sciences and humanities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:py36]",
      "language": "python",
      "name": "conda-env-py36-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "shell-scripting.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}